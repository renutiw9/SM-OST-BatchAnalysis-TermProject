\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}

\title{Batch Analysis of Network Security Monitoring Data}
\author{Renu Tiwari, Jiang Xing Kun,Mhyar Kousa}
\date{January 2021}

\begin{document}

\maketitle

\section{Duties Description performed by Project Team Members}

\subsection{Renu Tiwari (UYFIND)}
\begin{itemize}
    \item Data preprocessing on Jupyter Notebook: Initial exploration of  data to understand the fields and get  summary statistics and visualizations using Python.
    \item Streaming .csv data using kafka into "streaming" topic.
     \item Show .csv data on UI localhost after kafka stream.
    \item Use Spark Consumer to read data and do filtration.
    \item Created function to filter on basis of only TCP protocol accepted and export data to mysql and mongoDB.
    \item Explore different databases.
    \item Created configuration setup for MongoDB and migrated from MySQL to MongoDB database.
    \item Conversion of JavaInputDStream to JavaRDD.
    \item Code repository management on GitHub: creating repository, reviewing pull requests and access management.
    \item Participating in project team meetings.
    \item Prepared latex report, duties and  presentation material.
    \end{itemize}

\subsection{Jiang Xing Kun (R0F1R5)}
\begin{itemize}
   \item Loading data into wire-shark and export to .csv data.
    \item Setting up Kafka Connect to read .csv data into 'topic1' stream
    \item Setting up Spark Kafka Stream Connection.
    \item Do filtration on basis of data length and export data to MySQL.
    \item Do setup for Flink Streaming.
    \item Prepare Dashboard.html for pie chart visualization.
    \item Pushing code contribution on GitHub.
     \item Participating in project team meetings, 
\end{itemize}

\subsection{Mhyar Kousa (ATTSGP)}
\begin{itemize}
\item Convert Pcap files into CVS data-set using wireshark.
    \item Data  Reprocessing on Data Bricks Environment and apply anomaly detection to detect the outliers.
    \item Streaming .csv data using kafka into “streaming” topic
    \item Reading the streaming data using apache spark.
    \item Preparing These series of RDDs that are feed into the Spark Mllib Pipeline containing the required reprocessing steps and the actual classifier model.
    \item Prepare MariaDB SQL Server to save  the data instances after reprocessing phase is completed.
    \item This database can be queried by a live dashboard, which displays in real time the results of the classification of the incoming data instances in dynamic charts.
     \item Participating in project team meetings, 
\end{itemize}
\end{document}